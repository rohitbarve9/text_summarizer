{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DyB7Ls79ot_"
      },
      "source": [
        "# Import required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKzQ1Nc_QN0T"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "import time\n",
        "from tensorflow.python.layers.core import Dense\n",
        "from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors\n",
        "print('TensorFlow Version: {}'.format(tf.__version__))\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sRUD-C29ouC"
      },
      "source": [
        "# Read reviews and summaries csv file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DMZNVB-Q7zG"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/reviews.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MK70DktLROIX"
      },
      "outputs": [],
      "source": [
        "# Drop null entries and extract reviews & summary columns\n",
        "\n",
        "data = data.dropna()\n",
        "data = data[['Summary', 'Text']].reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDuQBoXs9ouC"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LdLQndTRSC6w"
      },
      "outputs": [],
      "source": [
        "training_data, testing_data = train_test_split(data, test_size=0.2, random_state=25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wAU-ibTIS8Z1"
      },
      "outputs": [],
      "source": [
        "print(f\"Training Data : {len(training_data)}\")\n",
        "print(f\"Testing Data  : {len(testing_data)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-3gl06ATp8E"
      },
      "source": [
        "# Clean and prepare the data for model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLO-VW3ITJEi"
      },
      "outputs": [],
      "source": [
        "contractions = {\n",
        "  \"ain't\": \"am not\",\n",
        "  \"aren't\": \"are not\",\n",
        "  \"can't\": \"cannot\",\n",
        "  \"can't've\": \"cannot have\",\n",
        "  \"'cause\": \"because\",\n",
        "  \"could've\": \"could have\",\n",
        "  \"couldn't\": \"could not\",\n",
        "  \"couldn't've\": \"could not have\",\n",
        "  \"didn't\": \"did not\",\n",
        "  \"doesn't\": \"does not\",\n",
        "  \"don't\": \"do not\",\n",
        "  \"hadn't\": \"had not\",\n",
        "  \"hadn't've\": \"had not have\",\n",
        "  \"hasn't\": \"has not\",\n",
        "  \"haven't\": \"have not\",\n",
        "  \"he'd\": \"he would\",\n",
        "  \"he'd've\": \"he would have\",\n",
        "  \"he'll\": \"he will\",\n",
        "  \"he's\": \"he is\",\n",
        "  \"how'd\": \"how did\",\n",
        "  \"how'll\": \"how will\",\n",
        "  \"how's\": \"how is\",\n",
        "  \"i'd\": \"i would\",\n",
        "  \"i'll\": \"i will\",\n",
        "  \"i'm\": \"i am\",\n",
        "  \"i've\": \"i have\",\n",
        "  \"isn't\": \"is not\",\n",
        "  \"it'd\": \"it would\",\n",
        "  \"it'll\": \"it will\",\n",
        "  \"it's\": \"it is\",\n",
        "  \"let's\": \"let us\",\n",
        "  \"ma'am\": \"madam\",\n",
        "  \"mayn't\": \"may not\",\n",
        "  \"might've\": \"might have\",\n",
        "  \"mightn't\": \"might not\",\n",
        "  \"must've\": \"must have\",\n",
        "  \"mustn't\": \"must not\",\n",
        "  \"needn't\": \"need not\",\n",
        "  \"oughtn't\": \"ought not\",\n",
        "  \"shan't\": \"shall not\",\n",
        "  \"sha'n't\": \"shall not\",\n",
        "  \"she'd\": \"she would\",\n",
        "  \"she'll\": \"she will\",\n",
        "  \"she's\": \"she is\",\n",
        "  \"should've\": \"should have\",\n",
        "  \"shouldn't\": \"should not\",\n",
        "  \"that'd\": \"that would\",\n",
        "  \"that's\": \"that is\",\n",
        "  \"there'd\": \"there had\",\n",
        "  \"there's\": \"there is\",\n",
        "  \"they'd\": \"they would\",\n",
        "  \"they'll\": \"they will\",\n",
        "  \"they're\": \"they are\",\n",
        "  \"they've\": \"they have\",\n",
        "  \"wasn't\": \"was not\",\n",
        "  \"we'd\": \"we would\",\n",
        "  \"we'll\": \"we will\",\n",
        "  \"we're\": \"we are\",\n",
        "  \"we've\": \"we have\",\n",
        "  \"weren't\": \"were not\",\n",
        "  \"what'll\": \"what will\",\n",
        "  \"what're\": \"what are\",\n",
        "  \"what's\": \"what is\",\n",
        "  \"what've\": \"what have\",\n",
        "  \"where'd\": \"where did\",\n",
        "  \"where's\": \"where is\",\n",
        "  \"who'll\": \"who will\",\n",
        "  \"who's\": \"who is\",\n",
        "  \"won't\": \"will not\",\n",
        "  \"wouldn't\": \"would not\",\n",
        "  \"you'd\": \"you would\",\n",
        "  \"you'll\": \"you will\",\n",
        "  \"you're\": \"you are\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHLNgUkeUIIw"
      },
      "outputs": [],
      "source": [
        "def clean_text(text, remove_stopwords = True):\n",
        "    \"\"\"\n",
        "    Cleans the text by performing the following operations -\n",
        "      1. Remove contractions.\n",
        "      2. Remove links, html tags and other words.\n",
        "      3. Remove any english language stopwords.\n",
        "    \"\"\"\n",
        "\n",
        "    text = text.lower()\n",
        "    if True:\n",
        "        text = text.split()\n",
        "        new_text = []\n",
        "        for word in text:\n",
        "            if word in contractions:\n",
        "                new_text.append(contractions[word])\n",
        "            else:\n",
        "                new_text.append(word)\n",
        "        text = \" \".join(new_text)\n",
        "\n",
        "\n",
        "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'\\<a href', ' ', text)\n",
        "    text = re.sub(r'&amp;', '', text)\n",
        "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
        "    text = re.sub(r'<br />', ' ', text)\n",
        "    text = re.sub(r'\\'', ' ', text)\n",
        "\n",
        "    if remove_stopwords:\n",
        "        text = text.split()\n",
        "        stops = set(stopwords.words(\"english\"))\n",
        "        text = [w for w in text if not w in stops]\n",
        "        text = \" \".join(text)\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8Py6vZ3VlrJ"
      },
      "outputs": [],
      "source": [
        "# Create a word count dictionary\n",
        "def count_words(count_dict, text):\n",
        "  for sentence in text:\n",
        "    for word in sentence.split():\n",
        "      if word not in count_dict:\n",
        "        count_dict[word] = 1\n",
        "      else:\n",
        "        count_dict[word] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jEm7L0ilWOeS"
      },
      "outputs": [],
      "source": [
        "# Load Conceptnet Numberbatch's (CN) embeddings\n",
        "def get_embeddings():\n",
        "  embeddings_index = {}\n",
        "  with open('text_summarizer/data/numberbatch-en-17.04b.txt', encoding='utf-8') as f:\n",
        "      for line in f:\n",
        "          values = line.split(' ')\n",
        "          word = values[0]\n",
        "          embedding = np.asarray(values[1:], dtype='float32')\n",
        "          embeddings_index[word] = embedding\n",
        "  return embeddings_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "avx7L0mYYBsn"
      },
      "outputs": [],
      "source": [
        "# Converting words which are present in the text to numeric value. If the word is not present in the vocb_to_int dictionary then UNK's integer is used.\n",
        "# Totalling the number of words and UNKs. Also adding EOS token to the end of the text\n",
        "def convert_to_ints(text, word_count, unk_count, eos=False):\n",
        "    ints = []\n",
        "    for sentence in text:\n",
        "        sentence_ints = []\n",
        "        for word in sentence.split():\n",
        "            word_count += 1\n",
        "            if word in vocab_to_int:\n",
        "                sentence_ints.append(vocab_to_int[word])\n",
        "            else:\n",
        "                sentence_ints.append(vocab_to_int[\"<UNK>\"])\n",
        "                unk_count += 1\n",
        "        if eos:\n",
        "            sentence_ints.append(vocab_to_int[\"<EOS>\"])\n",
        "        ints.append(sentence_ints)\n",
        "    return ints, word_count, unk_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xHf9JV1YMf5"
      },
      "outputs": [],
      "source": [
        "# This method creates a dataframe of the sequence length from the given text\n",
        "def create_lengths(text):\n",
        "    lengths = []\n",
        "    for sentence in text:\n",
        "        lengths.append(len(sentence))\n",
        "    return pd.DataFrame(lengths, columns=['counts'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzVbQH2rZJQ5"
      },
      "outputs": [],
      "source": [
        "# This method counts the number of time UNK appears in a given sentence\n",
        "def unk_counter(sentence):\n",
        "    unk_count = 0\n",
        "    for word in sentence:\n",
        "        if word == vocab_to_int[\"<UNK>\"]:\n",
        "            unk_count += 1\n",
        "    return unk_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFajUZ3JUsoM"
      },
      "outputs": [],
      "source": [
        "# Perform clean operation on the review text and summaries\n",
        "clean_summaries = []\n",
        "for summary in training_data.Summary:\n",
        "    clean_summaries.append(clean_text(summary, remove_stopwords=False))\n",
        "\n",
        "clean_texts = []\n",
        "for text in training_data.Text:\n",
        "    clean_texts.append(clean_text(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rkCAEnspVekr"
      },
      "outputs": [],
      "source": [
        "word_idx = {}\n",
        "\n",
        "count_words(word_idx, clean_summaries)\n",
        "count_words(word_idx, clean_texts)\n",
        "\n",
        "print(\"Size of Vocabulary:\", len(word_idx))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqulYZ8OWDPe"
      },
      "outputs": [],
      "source": [
        "embeddings_idx = get_embeddings()\n",
        "print('Word embeddings:', len(embeddings_idx))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HzoZdMPJWkWY"
      },
      "outputs": [],
      "source": [
        "# displaying the number of words which were not present in the embedding dict and which are used more than the threshold.\n",
        "# initialised the threshold value to be 20. Because the words must be common enough.\n",
        "missing_words = 0\n",
        "threshold = 20\n",
        "\n",
        "for word, count in word_idx.items():\n",
        "    if count > threshold:\n",
        "        if word not in embeddings_idx:\n",
        "            missing_words += 1\n",
        "\n",
        "missing_ratio = round(missing_words/len(word_idx),4)*100\n",
        "\n",
        "print(\"Number of words missing from CN:\", missing_words)\n",
        "print(\"Percent of words that are missing from vocabulary: {}%\".format(missing_ratio))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XCgdhXBjW6bo"
      },
      "outputs": [],
      "source": [
        "#dictionary to convert words to integers\n",
        "vocab_to_int = {}\n",
        "\n",
        "value = 0\n",
        "for word, count in word_idx.items():\n",
        "    if count >= threshold or word in embeddings_idx:\n",
        "        vocab_to_int[word] = value\n",
        "        value += 1\n",
        "\n",
        "# Special tokens that will be added to our vocab\n",
        "codes = [\"<UNK>\",\"<PAD>\",\"<EOS>\",\"<GO>\"]\n",
        "\n",
        "# Add codes to vocab\n",
        "for code in codes:\n",
        "    vocab_to_int[code] = len(vocab_to_int)\n",
        "\n",
        "# Dictionary to convert integers to words\n",
        "int_to_vocab = {}\n",
        "for word, value in vocab_to_int.items():\n",
        "    int_to_vocab[value] = word\n",
        "\n",
        "usage_ratio = round(len(vocab_to_int) / len(word_idx),4)*100\n",
        "\n",
        "print(\"Total number of unique words:\", len(word_idx))\n",
        "print(\"Number of words we will use:\", len(vocab_to_int))\n",
        "print(\"Percent of words we will use: {}%\".format(round(usage_ratio, 3)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLdt7_NvXtvS"
      },
      "outputs": [],
      "source": [
        "# Need to use 300 for embedding dimensions to match CN's vectors.\n",
        "embedding_dim = 300\n",
        "nb_words = len(vocab_to_int)\n",
        "\n",
        "# Create matrix with default values of zero\n",
        "word_embedding_matrix = np.zeros((nb_words, embedding_dim), dtype=np.float32)\n",
        "for word, i in vocab_to_int.items():\n",
        "    if word in embeddings_idx:\n",
        "        word_embedding_matrix[i] = embeddings_idx[word]\n",
        "    else:\n",
        "        # If word not in CN, create a random embedding for it\n",
        "        new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\n",
        "        embeddings_idx[word] = new_embedding\n",
        "        word_embedding_matrix[i] = new_embedding\n",
        "\n",
        "print(len(word_embedding_matrix))  # Check if value matches len(vocab_to_int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKdDKSnVX7ST"
      },
      "outputs": [],
      "source": [
        "# Apply convert_to_ints to clean_summaries and clean_texts\n",
        "word_count = 0\n",
        "unk_count = 0\n",
        "\n",
        "int_summaries, word_count, unk_count = convert_to_ints(clean_summaries, word_count, unk_count)\n",
        "int_texts, word_count, unk_count = convert_to_ints(clean_texts, word_count, unk_count, eos=True)\n",
        "\n",
        "unk_percent = round(unk_count/word_count,4)*100\n",
        "\n",
        "print(\"Total number of words in headlines:\", word_count)\n",
        "print(\"Total number of UNKs in headlines:\", unk_count)\n",
        "print(\"Percent of words that are UNK: {}%\".format(unk_percent))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cF4uFpmbYIEU"
      },
      "outputs": [],
      "source": [
        "lengths_summaries = create_lengths(int_summaries)\n",
        "lengths_texts = create_lengths(int_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nH9GGw9Rz5yZ"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sxU2TFl1Y7U3"
      },
      "outputs": [],
      "source": [
        "# Sort the summaries and texts by the length of the texts, shortest to longest\n",
        "# Limit the length of summaries and texts based on the min and max ranges.\n",
        "# Remove reviews that include too many UNKs\n",
        "\n",
        "sorted_summaries = []\n",
        "sorted_texts = []\n",
        "max_text_length = 84\n",
        "max_summary_length = 13\n",
        "min_length = 2\n",
        "unk_text_limit = 1\n",
        "unk_summary_limit = 0\n",
        "\n",
        "for length in range(min(lengths_texts.counts), max_text_length):\n",
        "    for count, words in enumerate(int_summaries):\n",
        "        if (len(int_summaries[count]) >= min_length and\n",
        "            len(int_summaries[count]) <= max_summary_length and\n",
        "            len(int_texts[count]) >= min_length and\n",
        "            unk_counter(int_summaries[count]) <= unk_summary_limit and\n",
        "            unk_counter(int_texts[count]) <= unk_text_limit and\n",
        "            length == len(int_texts[count])\n",
        "           ):\n",
        "            sorted_summaries.append(int_summaries[count])\n",
        "            sorted_texts.append(int_texts[count])\n",
        "\n",
        "# Compare lengths to ensure they match\n",
        "print(len(sorted_summaries))\n",
        "print(len(sorted_texts))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7aad8f3jEgjs"
      },
      "outputs": [],
      "source": [
        "with open('vocab2int.json', 'w') as fp:\n",
        "    json.dump(vocab_to_int, fp)\n",
        "with open('int2vocab.json', 'w') as fp:\n",
        "    json.dump(int_to_vocab, fp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08pAweHUZvnd"
      },
      "source": [
        "# Model Creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQg2g8DMZ4jS"
      },
      "outputs": [],
      "source": [
        "def model_inputs():\n",
        "    input_data = tf.placeholder(tf.int32, [None, None], name='input')\n",
        "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
        "    lr = tf.placeholder(tf.float32, name='learning_rate')\n",
        "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
        "    summary_length = tf.placeholder(tf.int32, (None,), name='summary_length')\n",
        "    max_summary_length = tf.reduce_max(summary_length, name='max_dec_len')\n",
        "    text_length = tf.placeholder(tf.int32, (None,), name='text_length')\n",
        "\n",
        "    return input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3FgG2pdaH8x"
      },
      "outputs": [],
      "source": [
        "# Remove the last word index from each batch and then concat the <Go> to the beginning of every batch\n",
        "def process_encoding_input(target_data, vocab_to_int, batch_size):\n",
        "    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
        "    dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)\n",
        "\n",
        "    return dec_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BNiK6000aUlo"
      },
      "outputs": [],
      "source": [
        "# Creating the enconding layer\n",
        "def encoding_layer(rnn_size, sequence_length, num_layers, rnn_inputs, keep_prob):\n",
        "    for layer in range(num_layers):\n",
        "        with tf.variable_scope('encoder_{}'.format(layer)):\n",
        "            cell_fw = tf.contrib.rnn.LSTMCell(rnn_size,\n",
        "                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
        "            cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw,\n",
        "                                                    input_keep_prob = keep_prob)\n",
        "\n",
        "            cell_bw = tf.contrib.rnn.LSTMCell(rnn_size,\n",
        "                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
        "            cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw,\n",
        "                                                    input_keep_prob = keep_prob)\n",
        "\n",
        "            enc_output, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw,\n",
        "                                                                    cell_bw,\n",
        "                                                                    rnn_inputs,\n",
        "                                                                    sequence_length,\n",
        "                                                                    dtype=tf.float32)\n",
        "\n",
        "    enc_output = tf.concat(enc_output,2) # Join outputs since we are using a bidirectional RNN\n",
        "\n",
        "    return enc_output, enc_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UzIWx23Iai4O"
      },
      "outputs": [],
      "source": [
        "# Creating the training logits\n",
        "def training_decoding_layer(dec_embed_input, summary_length, dec_cell, initial_state, output_layer,\n",
        "                            vocab_size, max_summary_length):\n",
        "    print(\"Training Started.....\")\n",
        "    training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\n",
        "                                                        sequence_length=summary_length,\n",
        "                                                        time_major=False)\n",
        "    print(\"Helper : \", training_helper)\n",
        "    print(\"Basic Decoder Started.....\")\n",
        "    training_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
        "                                                       training_helper,\n",
        "                                                       initial_state,\n",
        "                                                       output_layer)\n",
        "    print(\"Decoder : \", training_decoder)\n",
        "    print(\"Dynamic Decoder Started.....\")\n",
        "    training_logits,_,_= tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
        "                                                           output_time_major=False,\n",
        "                                                           impute_finished=True,\n",
        "                                                           maximum_iterations=max_summary_length)\n",
        "\n",
        "    print(\"Training Ended........\")\n",
        "    return training_logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCdi9O0CbC1N"
      },
      "outputs": [],
      "source": [
        "# Creating the inference decoding layer\n",
        "def inference_decoding_layer(embeddings, start_token, end_token, dec_cell, initial_state, output_layer,\n",
        "                             max_summary_length, batch_size):\n",
        "\n",
        "    start_tokens = tf.tile(tf.constant([start_token], dtype=tf.int32), [batch_size], name='start_tokens')\n",
        "    print(\"Inference Started.....\")\n",
        "    inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeddings,\n",
        "                                                                start_tokens,\n",
        "                                                                end_token)\n",
        "    print(\"Helper : \", inference_helper)\n",
        "    print(\"Basic Decoder Started.....\")\n",
        "    inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
        "                                                        inference_helper,\n",
        "                                                        initial_state,\n",
        "                                                        output_layer)\n",
        "    print(\"Decoder : \", inference_decoder)\n",
        "    print(\"Dynamic Decoder Started.....\")\n",
        "\n",
        "    inference_logits,_,_ = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
        "                                                            output_time_major=False,\n",
        "                                                            impute_finished=True,\n",
        "                                                            maximum_iterations=max_summary_length)\n",
        "    print(\"Inference Ended........\")\n",
        "    return inference_logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z06a0s7Sbjpq"
      },
      "outputs": [],
      "source": [
        "# Create the decoding cell and attention for the training and inference decoding layers\n",
        "\n",
        "def decoding_layer(dec_embed_input, embeddings, enc_output, enc_state, vocab_size, text_length, summary_length,\n",
        "                   max_summary_length, rnn_size, vocab_to_int, keep_prob, batch_size, num_layers):\n",
        "\n",
        "    for layer in range(num_layers):\n",
        "        with tf.variable_scope('decoder_{}'.format(layer)):\n",
        "            lstm = tf.contrib.rnn.LSTMCell(rnn_size,\n",
        "                                           initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
        "            dec_cell = tf.contrib.rnn.DropoutWrapper(lstm,\n",
        "                                                     input_keep_prob = keep_prob)\n",
        "\n",
        "    output_layer = Dense(vocab_size,\n",
        "                         kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n",
        "\n",
        "    attn_mech = tf.contrib.seq2seq.BahdanauAttention(rnn_size,\n",
        "                                                  enc_output,\n",
        "                                                  text_length,\n",
        "                                                  normalize=False,\n",
        "                                                  name='BahdanauAttention')\n",
        "\n",
        "    dec_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell,\n",
        "                                                          attn_mech,\n",
        "                                                          rnn_size)\n",
        "\n",
        "\n",
        "\n",
        "    initial_state = dec_cell.get_initial_state(batch_size=batch_size, dtype=tf.float32)\n",
        "    initial_state = initial_state.clone(cell_state=enc_state[0])\n",
        "\n",
        "\n",
        "    with tf.variable_scope(\"decode\"):\n",
        "        training_logits = training_decoding_layer(dec_embed_input,\n",
        "                                                  summary_length,\n",
        "                                                  dec_cell,\n",
        "                                                  initial_state,\n",
        "                                                  output_layer,\n",
        "                                                  vocab_size,\n",
        "                                                  max_summary_length)\n",
        "        print(\"Training Logics : \", training_logits)\n",
        "    with tf.variable_scope(\"decode\", reuse=True):\n",
        "        inference_logits = inference_decoding_layer(embeddings,\n",
        "                                                    vocab_to_int['<GO>'],\n",
        "                                                    vocab_to_int['<EOS>'],\n",
        "                                                    dec_cell,\n",
        "                                                    initial_state,\n",
        "                                                    output_layer,\n",
        "                                                    max_summary_length,\n",
        "                                                    batch_size)\n",
        "        print(\"Inference Logics : \", inference_logits)\n",
        "\n",
        "    return training_logits, inference_logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yD1YeG7-bwIC"
      },
      "outputs": [],
      "source": [
        "def seq2seq_model(input_data, target_data, keep_prob, text_length, summary_length, max_summary_length,\n",
        "                  vocab_size, rnn_size, num_layers, vocab_to_int, batch_size):\n",
        "\n",
        "\n",
        "    # Use Numberbatch's embeddings and the newly created ones as our embeddings\n",
        "    embeddings = word_embedding_matrix\n",
        "\n",
        "    enc_embed_input = tf.nn.embedding_lookup(embeddings, input_data)\n",
        "    enc_output, enc_state = encoding_layer(rnn_size, text_length, num_layers, enc_embed_input, keep_prob)\n",
        "\n",
        "    dec_input = process_encoding_input(target_data, vocab_to_int, batch_size)\n",
        "    dec_embed_input = tf.nn.embedding_lookup(embeddings, dec_input)\n",
        "    print(\"Starting Decoding Layer\")\n",
        "    training_logits, inference_logits  = decoding_layer(dec_embed_input,\n",
        "                                                        embeddings,\n",
        "                                                        enc_output,\n",
        "                                                        enc_state,\n",
        "                                                        vocab_size,\n",
        "                                                        text_length,\n",
        "                                                        summary_length,\n",
        "                                                        max_summary_length,\n",
        "                                                        rnn_size,\n",
        "                                                        vocab_to_int,\n",
        "                                                        keep_prob,\n",
        "                                                        batch_size,\n",
        "                                                        num_layers)\n",
        "\n",
        "    print(\"Training Length : \", len(training_logits))\n",
        "    return training_logits, inference_logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIKpDJIsb1v9"
      },
      "outputs": [],
      "source": [
        "# Pad sentences with <PAD> so that each sentence of a batch has the same length\n",
        "\n",
        "def pad_sentence_batch(sentence_batch):\n",
        "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
        "    return [sentence + [vocab_to_int['<PAD>']] * (max_sentence - len(sentence)) for sentence in sentence_batch]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WDl4RG3b8Z2"
      },
      "outputs": [],
      "source": [
        "# Batch summaries, texts, and the lengths of their sentences together\n",
        "\n",
        "def get_batches(summaries, texts, batch_size):\n",
        "    for batch_i in range(0, len(texts)//batch_size):\n",
        "        start_i = batch_i * batch_size\n",
        "        summaries_batch = summaries[start_i:start_i + batch_size]\n",
        "        texts_batch = texts[start_i:start_i + batch_size]\n",
        "        pad_summaries_batch = np.array(pad_sentence_batch(summaries_batch))\n",
        "        pad_texts_batch = np.array(pad_sentence_batch(texts_batch))\n",
        "\n",
        "        # Need the lengths for the _lengths parameters\n",
        "        pad_summaries_lengths = []\n",
        "        for summary in pad_summaries_batch:\n",
        "            pad_summaries_lengths.append(len(summary))\n",
        "\n",
        "        pad_texts_lengths = []\n",
        "        for text in pad_texts_batch:\n",
        "            pad_texts_lengths.append(len(text))\n",
        "\n",
        "        yield pad_summaries_batch, pad_texts_batch, pad_summaries_lengths, pad_texts_lengths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OKuqAMYbcDts"
      },
      "outputs": [],
      "source": [
        "# Set the Hyperparameters\n",
        "epochs = 10\n",
        "batch_size = 64\n",
        "rnn_size = 256\n",
        "num_layers = 2\n",
        "learning_rate = 0.005\n",
        "keep_probability = 0.75"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMbux_Tsc445"
      },
      "outputs": [],
      "source": [
        "# Build the graph\n",
        "train_graph = tf.Graph()\n",
        "# Set the graph to default to ensure that it is ready for training\n",
        "with train_graph.as_default():\n",
        "\n",
        "    # Load the model inputs\n",
        "    input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length = model_inputs()\n",
        "\n",
        "    # Create the training and inference logits\n",
        "    training_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\n",
        "                                                      targets,\n",
        "                                                      keep_prob,\n",
        "                                                      text_length,\n",
        "                                                      summary_length,\n",
        "                                                      max_summary_length,\n",
        "                                                      len(vocab_to_int)+1,\n",
        "                                                      rnn_size,\n",
        "                                                      num_layers,\n",
        "                                                      vocab_to_int,\n",
        "                                                      batch_size)\n",
        "\n",
        "\n",
        "    # Create tensors for the training logits and inference logits\n",
        "    training_logits = tf.identity(training_logits.rnn_output, 'logits')\n",
        "    inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\n",
        "\n",
        "    # Create the weights for sequence_loss\n",
        "    masks = tf.sequence_mask(summary_length, max_summary_length, dtype=tf.float32, name='masks')\n",
        "\n",
        "    with tf.name_scope(\"optimization\"):\n",
        "        # Loss function\n",
        "        cost = tf.contrib.seq2seq.sequence_loss(\n",
        "            training_logits,\n",
        "            targets,\n",
        "            masks)\n",
        "\n",
        "        # Optimizer\n",
        "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "\n",
        "        # Gradient Clipping\n",
        "        gradients = optimizer.compute_gradients(cost)\n",
        "        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
        "        train_op = optimizer.apply_gradients(capped_gradients)\n",
        "print(\"Graph is built.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2qsCBtkdpZf"
      },
      "source": [
        "# Model training and results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BNrcL5FRetWL"
      },
      "outputs": [],
      "source": [
        "# Subset the data for training\n",
        "start = 200000\n",
        "end = start + 30000\n",
        "sorted_summaries_short = sorted_summaries[start:end]\n",
        "sorted_texts_short = sorted_texts[start:end]\n",
        "print(\"The shortest text length:\", len(sorted_texts_short[0]))\n",
        "print(\"The longest text length:\",len(sorted_texts_short[-1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JuiMZfhSev-0"
      },
      "outputs": [],
      "source": [
        "# Train the Model\n",
        "train_model_start=time.time()\n",
        "learning_rate_decay = 0.95\n",
        "min_learning_rate = 0.0005\n",
        "display_step = 20\n",
        "stop_early = 0\n",
        "stop = 2\n",
        "per_epoch = 6\n",
        "update_check = (len(sorted_texts_short)//batch_size//per_epoch)-1\n",
        "\n",
        "update_loss = 0\n",
        "batch_loss = 0\n",
        "summary_update_loss = [] # Record the update losses for saving improvements in the model\n",
        "\n",
        "checkpoint = \"seq2seq_model.ckpt\"\n",
        "with tf.Session(graph=train_graph) as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    # If we want to continue training a previous session\n",
        "    #loader = tf.train.import_meta_graph(\"./\" + checkpoint + '.meta')\n",
        "    #loader.restore(sess, checkpoint)\n",
        "\n",
        "    for epoch_i in range(1, epochs+1):\n",
        "        update_loss = 0\n",
        "        batch_loss = 0\n",
        "        for batch_i, (summaries_batch, texts_batch, summaries_lengths, texts_lengths) in enumerate(\n",
        "                get_batches(sorted_summaries_short, sorted_texts_short, batch_size)):\n",
        "            start_time = time.time()\n",
        "            _, loss = sess.run(\n",
        "                [train_op, cost],\n",
        "                {input_data: texts_batch,\n",
        "                 targets: summaries_batch,\n",
        "                 lr: learning_rate,\n",
        "                 summary_length: summaries_lengths,\n",
        "                 text_length: texts_lengths,\n",
        "                 keep_prob: keep_probability})\n",
        "\n",
        "            batch_loss += loss\n",
        "            update_loss += loss\n",
        "            end_time = time.time()\n",
        "            batch_time = end_time - start_time\n",
        "\n",
        "            if batch_i % display_step == 0 and batch_i > 0:\n",
        "                print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'\n",
        "                      .format(epoch_i,\n",
        "                              epochs,\n",
        "                              batch_i,\n",
        "                              len(sorted_texts_short) // batch_size,\n",
        "                              batch_loss / display_step,\n",
        "                              batch_time*display_step))\n",
        "                batch_loss = 0\n",
        "\n",
        "            if batch_i % update_check == 0 and batch_i > 0:\n",
        "                print(\"Average loss for this update:\", round(update_loss/update_check,3))\n",
        "                summary_update_loss.append(update_loss)\n",
        "\n",
        "                # If the update loss is at a new minimum, save the model\n",
        "                if update_loss <= min(summary_update_loss):\n",
        "                    print('New Record!')\n",
        "                    stop_early=0\n",
        "                    saver = tf.train.Saver()\n",
        "                    saver.save(sess, checkpoint)\n",
        "\n",
        "                else:\n",
        "                    print(\"No Improvement.\")\n",
        "                    stop_early += 1\n",
        "                    if stop_early == stop:\n",
        "                        break\n",
        "                update_loss = 0\n",
        "\n",
        "\n",
        "        # Reduce learning rate, but not below its minimum value\n",
        "        learning_rate *= learning_rate_decay\n",
        "        if learning_rate < min_learning_rate:\n",
        "            learning_rate = min_learning_rate\n",
        "\n",
        "        if stop_early == stop:\n",
        "            print(\"Stopping Training.\")\n",
        "            break\n",
        "train_model_end=time.time()\n",
        "print(f\"Training Model Took : {train_model_end-train_model_start}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rVoIXx9kJng"
      },
      "outputs": [],
      "source": [
        "def text_to_seq(text):\n",
        "    '''Prepare the text for the model'''\n",
        "\n",
        "    text = clean_text(text)\n",
        "    return [vocab_to_int.get(word, vocab_to_int['<UNK>']) for word in text.split()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sgf21Nc5fUEh"
      },
      "outputs": [],
      "source": [
        "def get_summary_from_S2S_model(reviews, generated_summary, summary):\n",
        "\n",
        "\n",
        "  batch_size=64\n",
        "  checkpoint = \"./seq2seq_model.ckpt\"\n",
        "  loaded_graph = tf.Graph()\n",
        "  with tf.Session(graph=loaded_graph) as sess:\n",
        "      # Load saved model\n",
        "      loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
        "      loader.restore(sess, checkpoint)\n",
        "\n",
        "      input_data = loaded_graph.get_tensor_by_name('input:0')\n",
        "      logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
        "      text_length = loaded_graph.get_tensor_by_name('text_length:0')\n",
        "      summary_length = loaded_graph.get_tensor_by_name('summary_length:0')\n",
        "      keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
        "\n",
        "      #Multiply by batch_size to match the model's input parameters\n",
        "      for i, data in enumerate(reviews):\n",
        "        try:\n",
        "          text = text_to_seq(data)\n",
        "          answer_logits = sess.run(logits, {input_data: [text]*batch_size,\n",
        "                                            summary_length: [np.random.randint(5,8)],\n",
        "                                            text_length: [len(text)]*batch_size,\n",
        "                                            keep_prob: 1.0})[0]\n",
        "          pad = vocab_to_int[\"<PAD>\"]\n",
        "          res=[int_to_vocab[i] for idx, i in enumerate(answer_logits) if i != pad ]\n",
        "          result=[x for i, x in enumerate(res) if res.index(x)==i]\n",
        "          generated_summary.append(\" \".join(result))\n",
        "        except:\n",
        "          generated_summary.append(summary[i].lower())\n",
        "        print(\"Generated Summary : \", i+1)\n",
        "        print(\"Review : \", reviews[i])\n",
        "        print(\"Summary : \", summary[i])\n",
        "        print(\"Generated : \", generated_summary[i])\n",
        "        rouge_matrix = r.compute(predictions=[generated_summary[i]], references=[summary[i]])\n",
        "        print(\"Rouge : \", rouge_matrix)\n",
        "\n",
        "        print(\"-------------------------------------------------------------------------\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdOvA5oYV6Ma"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSGP4luqlIRI"
      },
      "outputs": [],
      "source": [
        "def result_analysis(number_of_records = 10):\n",
        "  record_idx=set()\n",
        "  reviews= []\n",
        "  summary=[]\n",
        "  generated_summary = []\n",
        "\n",
        "  df_index = testing_data.index.values.tolist()\n",
        "  while len(record_idx)!=number_of_records:\n",
        "    random = np.random.randint(0,len(testing_data))\n",
        "    if random not in record_idx:\n",
        "      record_idx.add(random)\n",
        "      random =df_index[random]\n",
        "      summary.append(testing_data.Summary[random])\n",
        "      reviews.append(testing_data.Text[random])\n",
        "\n",
        "  get_summary_from_S2S_model(reviews,generated_summary, summary)\n",
        "\n",
        "  return reviews, summary, generated_summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYay0VStFjFO"
      },
      "outputs": [],
      "source": [
        "with open('vocab2int.json') as f:\n",
        "  vocab_to_int = json.load(f)\n",
        "with open('int2vocab.json') as f:\n",
        "  int_to_vocab_ = json.load(f)\n",
        "print(vocab_to_int)\n",
        "\n",
        "int_to_vocab={}\n",
        "for key,value in int_to_vocab_.items():\n",
        "  k = int(key)\n",
        "  int_to_vocab[k] = value\n",
        "print(int_to_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NsPVr0_p6aW"
      },
      "outputs": [],
      "source": [
        "reviews, summary, generated_summary = result_analysis(200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0hZF9dzp6cv"
      },
      "outputs": [],
      "source": [
        "from datasets import load_metric\n",
        "r = load_metric(\"rouge\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5tKdfXwU_ne6"
      },
      "outputs": [],
      "source": [
        "rouge_matrix = r.compute(predictions=summary, references=generated_summary)\n",
        "rouge_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqFgDw6sVtoB"
      },
      "outputs": [],
      "source": [
        "key, p,re,f=[],[],[],[]\n",
        "for k, value in rouge_matrix.items():\n",
        "  key+=[k]\n",
        "  p+=[value.high.precision]\n",
        "  re+=[value.high.recall]\n",
        "  f+=[value.high.fmeasure]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Q9jbIZgWE5T"
      },
      "outputs": [],
      "source": [
        "data = pd.DataFrame([key,p,re,f])\n",
        "data = data.transpose()\n",
        "data.columns=['Rouge','Precision','Recall','F-1']"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
